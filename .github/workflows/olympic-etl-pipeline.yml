name: Olympic ETL Pipeline

on:
  push:
    branches: [ main, develop ]
  schedule:
    - cron: '0 2 * * *'  # Rodar diariamente √†s 2AM UTC
  workflow_dispatch:

env:
  PYTHON_VERSION: '3.11'
  GCP_PROJECT: my-olympic-etl
  BQ_DATASET: olympics_dataset
  BQ_TABLE: athletes

jobs:
  validate-and-test:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements-local.txt
        pip install pandas great-expectations
    
    - name: Run ETL Pipeline
      run: |
        python src/beam/pipelines/olympic_etl_simple.py
    
    - name: Export Data
      run: |
        python src/beam/pipelines/data_export.py
    
    - name: Verify output files
      run: |
        ls -lah output/
        if [ ! -f output/athletes.csv ]; then
          echo "‚ùå ERROR: athletes.csv not created"
          exit 1
        fi
        echo "‚úÖ Athletes data validated"
    
    - name: Upload Artifacts
      uses: actions/upload-artifact@v4
      with:
        name: olympic-data
        path: output/
        retention-days: 30

  bigquery-load:
    needs: validate-and-test
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Authenticate to Google Cloud
      uses: google-github-actions/auth@v1
      with:
        credentials_json: ${{ secrets.GCP_SA_KEY }}
    
    - name: Set up Google Cloud SDK
      uses: google-github-actions/setup-gcloud@v1
      with:
        version: latest
    
    - name: Configure gcloud
      run: |
        gcloud config set project ${{ env.GCP_PROJECT }}
    
    - name: Download Artifacts
      uses: actions/download-artifact@v4
      with:
        name: olympic-data
        path: output/
    
    - name: Load to BigQuery
      run: |
        bq load \
          --source_format=CSV \
          --skip_leading_rows=1 \
          --allow_jagged_rows \
          --replace \
          ${{ env.BQ_DATASET }}.${{ env.BQ_TABLE }} \
          output/athletes.csv \
          athlete_id:STRING,name:STRING,country:STRING,sport:STRING,medal:STRING,medal_rank:INTEGER,year:INTEGER,_processed_at:TIMESTAMP,_pipeline_version:STRING,_environment:STRING
    
    - name: Verify Load
      run: |
        bq query --use_legacy_sql=false \
          "SELECT COUNT(*) as total FROM \`${{ env.GCP_PROJECT }}.${{ env.BQ_DATASET }}.${{ env.BQ_TABLE }}\`"

  notify:
    name: Pipeline Summary
    needs: [validate-and-test, bigquery-load]
    runs-on: ubuntu-latest
    if: always()
    
    steps:
    - name: Pipeline Complete
      run: |
        echo "üèÖ Olympic Games ETL Pipeline Execution Summary"
        echo "================================================"
        echo "‚úÖ Validation Status: ${{ needs.validate-and-test.result }}"
        echo "‚úÖ BigQuery Load Status: ${{ needs.bigquery-load.result }}"
        echo ""
        echo "üìä Data Location:"
        echo "   Project: my-olympic-etl"
        echo "   Dataset: olympics_dataset"
        echo "   Table: athletes"
        echo ""
        echo "üìà Dashboard: Looker Studio"
        echo "üïê Next Run: Daily at 2:00 AM UTC"
      if: ${{ needs.validate-and-test.result == 'success' }}
    
    - name: Notify Failure
      run: |
        echo "‚ùå Pipeline failed!"
        echo "Check logs for details: https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}"
      if: ${{ failure() }}
